---
title: Urllib 库
categories: 
  - Python
tags: 
  - 爬虫
date: 2019-09-09 00:00:00
permalink: /pages/529679/
---

# Urllib 库

Python 内置的 HTTP 请求库

`urllib`库是Python中一个基础的网络请求库。可以模拟浏览器的行为，向指定的服务器发送一个请求，并可以保存服务器返回的数据。

## url详解
`URL`是`Uniform Resource Locator`的简写,统一资源定位符。

```
scheme://host:port/path/?query-string=xx#anchor
```
- **scheme**: 代表是访问的协议，一般为http或者https以及ftp等。
- **host**: 主机名，域名，比如baidu.com。
- **port**: 端口号。但你访问一个网址的时候，浏览器默认使用80端口。
- **path**: 查找路径。 比如ux34.cn/admin/，后面的admin/就是path。
- **query-string**: 查询字符串，比如www.baidu.com/s?wd=python,后面的wd=python就是查询字符串。
- **anchor**: 锚点，后台一般不用管，前端用来做页面定位的。

在浏览器中请求一个url，浏览器会对这个url进行一个编码。除英文字母，数字和部分符号外，其他的全部使用百分号+十六进制码值进行编码

## urlopen函数:
在Python3的urllib库中，所有和网络请求相关的方法。都被集到urllib.request模块下面了，先来看下urlopen函数基本的使用:
```python
from urllib import request
resp = request.urlopen("http://www.ux34.cn")
print(resp.read())
```
实际上，使用浏览器访问这个网站，右键查看源代码。你会发现，跟这个 print 函数打印出来的数据是一模一样的。也就是说，上面的三行代码就已经帮我们把这个网站的首页的全部代码爬下来了。一个基本的url请求对应的 python 代码真的非常简单。

1.url : 请求的url。
2.data : 请求的data,如果设置了这个值，那么将变成post请求。
3.返回值 : 返回值是一个`http.client.HTTPResposne`对象，这个对象是一个类文件句柄对象。
有read(size)、readline、readlines、以及getcode等方法。

## urlretrieve函数:
这个函数可以方便将网页上的一个文件保存到本地。以下代码非常方便的将我博客的封面下载到本地：
```python
from urllib import request

# 下载图片
# urlretrieve(url,'保存到本地的文件名')
request.urlretrieve('http://xxxx/xxx.jpg','1.jpg')
```

## urlencode函数
用浏览器发送请求的时候，如果url中包含中文或者其他特殊字符，那么浏览器会自动的给我们进行编码。而如果使用代码发送请求，那么就必须 手动进行编码，这时候就应该使用urlencode函数实现。urlencode可以把字典数据转为URL编码的数据。示例代码如下：
```python
from urllib import parse
data = {"name":"爬虫","greet":"hello world","age":20}
qs = parse.urlencode(data)
print(qs)

# name=%E7%88%AC%E8%99%AB&greet=hello+world&age=20
```

## parse_qs函数
可以将经过编码后的url参数进行解码。示例代码如下：
```python
from urllib import parse
data = {"name":"爬虫","greet":"hello world","age":20}
qs = parse.urlencode(data)
print(qs)
print(parse.parse_qs(qs))

# name=%E7%88%AC%E8%99%AB&greet=hello+world&age=20
# {'name': ['爬虫'], 'greet': ['hello world'], 'age': ['20']}
```

## urlparse和urlsplit
有时候拿到一个url，想要对这个url的各个组成部分进行分割，那么这时候就可以使用urlparse或者是urlsplit进行分割。示例代码入下：
```python
from urllib import request,parse

url = "http://www.baidu.com/s?username=zhiliao"

result = parse.urlsplit(url)
# result = parse.urlparse(url)

print('scheme:',result.scheme)
print('netloc:',result.netloc)
print('path:',result.path)
print('query:',result.query)

# scheme: http
# netloc: www.baidu.com
# path: /s
# query: username=zhiliao
```
区别: urlsplit比urlparse少了一个params属性
`http://www.baidu.com/s;hello?username=zhiliao`
params基本用不上，可以获取url中分号`;`后面的字符`hello`



## requset.Request类

如果想要在请求的时候增加一些请求头，那么就必须使用requset.Request类来实现。比如要增加一个user.Agent,示例代码如下:
```python
from urllib import request

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36"
}
req = request.Request("http://www.baidu.com/",headers=headers)
resp = request.urlopen(req)
print(resp.read())
```

## 用Request爬取拉勾网职位信息
```python
# 该代码已失效
from urllib import request,parse

url = "https://www.lagou.com/jobs/positionAjax.json?xl=%E6%9C%AC%E7%A7%91&px=default&city=%E6%B7%B1%E5%9C%B3&needAddtionalResult=false"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36",
    "Referer": "https://www.lagou.com/jobs/list_web%E5%89%8D%E7%AB%AF?px=default&xl=%E6%9C%AC%E7%A7%91&city=%E6%B7%B1%E5%9C%B3"
}
data = {
    "first":"true",
    "pn":1,
    "kd":"web前端"
}
req = request.Request(url,headers=headers,data=parse.urlencode(data).encode('utf-8'),method='POST')
resp = request.urlopen(req)
print(resp.read().decode('utf-8'))
```
一些网站对爬虫做了一些限制，要通过加请求头等方式来伪装更真实的浏览器请求
