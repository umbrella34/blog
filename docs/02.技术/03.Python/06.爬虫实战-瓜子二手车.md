---
title: 爬虫实战-瓜子二手车
date: 2020-12-22 20:13:01
permalink: /pages/3c86e5/
categories: 
  - 技术
  - Python
tags: 
  - 爬虫
  - 数据分析
---
# 爬虫实战-瓜子二手车

这学期学python数据分析，数据分析首先要有数据。

通过简单的学习，写了个爬虫爬取瓜子二手车网站数据。

### 使用技术

软件环境：python3.8 + vscode + Anaconda

数据分析库包

- python网络爬虫：requests 和 lxml
- 数据分析：NumPy、Pandas、re
- 数据可视化：echarts

### 爬虫思路

- 先爬目录（详情页的链接）

  ![image-20201222212549581](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222212549581.png)

  ![image-20201222212928120](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222212928120.png)

- 访问详情页链接爬取数据

  ![image-20201222212750707](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222212750707.png)

- 把爬到的数据存入csv文件，你也可以存入到json、excel，甚至是数据库

  花了快1天，差不多爬了2.6w条数据

  ![image-20201222213034189](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222213034189.png)

- 数据清洗与数据转换

- pandas，numpy 数据分析

- echarts可视化

### 遇到的问题：

- 最大页数只有50页

  网站显示有5w多条数据，但是分页只给50页，每页就40条数据，加起来才2000，远远达不到需要的量。首先我想到分城市来爬取目录，每个城市爬2000，然后发现换城市推荐的车基本都是重复的。后面改成分车的品牌来爬目录，勉强解决这个问题。但是这个网站没办法爬取完整数据。

- 爬虫中断续爬

  爬数据不是一朝一夕就能爬完的，中间也会遇到很多问题，比如断网了，cookie过期了，程序错误中断。总不能一错就重新开始爬吧，所以我通过读写json来保存最新进度。

  ```python
  def init():
      # 读取上次进度 {执行到第几个品牌，第几条数据，完成状态}
      global config
      try:
          with open('./data/getURL.config.json', 'r', encoding='utf-8') as f:
              config = json.load(f)
      except:
          with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
              # 没有配置文件重新写一个
              config = {
                  "brand": 0,
                  "count": 0,
                  "status": -1
              }
              f.write(json.dumps(config))
  
      if config["status"] != -1:
          print('检测到【获取详情页面Url】任务已全部完成, 程序退出...')
          # 强行退出程序
          sys.exit()
  
  
  def saveProgress():
      # 覆盖存档
      with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
          f.write(json.dumps(config))
          
  # 每写入一条数据，修改一下全局的config变量，再执行saveProgress函数
  # 如每爬完1页数据时就这样保存进度
  # 当品牌更换时记得把计数count重置为0
  config['count'] += 1
  saveProgress()
  
  ```

- 异常处理

  由于各种因素，程序不可避免错误，但也不能让它停止，所以使用`try except`来处理错误。因为网络问题，总有几条链接每成功访问，或者说爬到的链接已经过期。所以记录下错误的每一条链接位置，方便查找错误，后期也可以再写个程序捡回来重爬。

  ```python
  try:
  	'...'
  except:
      with open('./data/ERROR.csv', 'a', encoding='utf-8') as f:
          f.write(某某品牌+','+str(第几行)+'\n')
  ```



## 爬虫部分

最终的项目结构

![项目结构](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201231111156151.png)

```
爬虫部分的项目结构

├── data      数据保存目录，请手动创建

├── 爬虫-获取详细页链接.py     爬取链接

├── 爬虫-获取详细页数据.py     爬取数据
```

```python
#爬虫-获取详细页链接.py

import requests # Python HTTP 库
import json # 读写进度
import sys # 用于中途退出程序
from lxml import etree # 用于解析字符串格式的HTML文档对象

# 请求头 自己登录一遍 F12 NetWork 复制过来
header = {
    "Cookie": "请重新获取",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
}

# 基础路径 瓜子二手车
BaseUrl = "https://www.guazi.com"

# 瓜子二手车 热门品牌中文和链接中的缩写
brand = ['大众','本田','别克','丰田','福特','日产','宝马','奔驰','现代','奥迪','雪佛兰','吉利','哈弗','马自达','起亚']
brandUrl = ['dazhong','honda','buick','toyota','ford','richan','bmw','benz','hyundai','audi','chevrolet','jili','hafu','mazda','kia']


def getUrls(url, pageIndex):
    # 获取详情页面url
    resp = requests.get(url, headers=header)
    text = resp.content.decode('utf-8')
    html = etree.HTML(text)
    maxPage = html.xpath('/html/body/div[6]/div[7]/ul/li/a/span/text()')[-2]
    # 超出最大页数返回None
    if pageIndex > int(maxPage):
        return None
    ul = html.xpath('//ul[@class="carlist clearfix js-top"]')[0]
    lis = ul.xpath('./li')
    urls = []
    for li in lis:
        url = li.xpath('./a/@href')
        url = BaseUrl+url[0]
        urls.append(url)
    return urls


def init():
    # 读取上次进度 {执行到第几个品牌，第几条数据，完成状态}
    global config
    try:
        with open('./data/getURL.config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    except:
        with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
            # 没有配置文件重新写一个
            config = {
                "brand": 0,
                "count": 0,
                "status": -1
            }
            f.write(json.dumps(config))

    if config["status"] != -1:
        print('检测到【获取详情页面Url】任务已全部完成, 程序退出...')
        # 强行退出程序
        sys.exit()


def saveProgress():
    # 覆盖存档
    with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
        f.write(json.dumps(config))


def main():
    init()
    for brandIndex in range(config['brand'], len(brand)):
        config['brand'] = brandIndex
        brandName = brand[brandIndex]
        for pageIndex in range(config['count'], 50):
            print('正在爬取【{}】第{}页数据链接'.format(brandName, pageIndex+1))
            with open('./data/{}-Urls.csv'.format(brandName), 'a', encoding='utf-8') as f:
                url = '{}/xm/{}/o{}/'.format(BaseUrl, brandUrl[brandIndex], pageIndex+1)
                data = getUrls(url, pageIndex+1)
                # 如果返回的是None 则改品牌没有50页 跳出
                if data is None:
                    break
                f.write('\n'.join(data)+'\n')
            # 每读一页数据保存一下存档
            config['count'] = pageIndex
            saveProgress()

        # 每读完一个城市 重置页数 保存一下存档
        config['count'] = 0
        saveProgress()

    config["status"] = 0
    saveProgress()
    print('【获取详情页面Url】任务已全部完成, 程序退出...')


if __name__ == "__main__":
    main()
```

```python
# 爬虫-获取详细页数据.py

import requests  # Python HTTP 库
import json  # 读写进度
import re  # 正则 用于截取链接部分作为ID
import sys  # 用于中途退出程序
import pathlib  # 用于判断文件是否存在
import pandas as pd  # 用于读取.csv链接数据
from lxml import etree  # 用于解析字符串格式的HTML文档对象

# 请求头 自己登录一遍 F12 NetWork 复制过来
# cookie 一段时间会过期 记的更换
header = {
    "Cookie": "请重新获取",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
}


# 瓜子二手车 热门品牌中文和链接中的缩写
brand = ['大众', '本田', '别克', '丰田', '福特', '日产', '宝马',
         '奔驰', '现代', '奥迪', '雪佛兰', '吉利', '哈弗', '马自达', '起亚']
brandUrl = ['dazhong', 'honda', 'buick', 'toyota', 'ford', 'richan', 'bmw',
            'benz', 'hyundai', 'audi', 'chevrolet', 'jili', 'hafu', 'mazda', 'kia']


# csv的表头
infoKey = ['ID', '价格', '品牌', '标题', '车主', '表显里程', '变速箱', '过户次数', '看车方式', '厂商', '级别', '发动机', '车身结构', '长/宽/高(mm)', '轴距(mm)', '行李箱容积(L)', '整备质量(kg)', '排量(L)', '进气形式', '气缸数', '最大马力(Ps)', '最大扭矩(N*m)', '燃料类型', '燃油标号', '供油方式', '驱动方式', '助力类型', '前悬挂类型', '后悬挂类型', '前制动类型', '后制动类型', '驱车制动类型', '前轮胎规格',
           '后轮胎规格', '主/副驾驶安全气囊', '前/后排侧气囊', '前/后排头部气囊', '胎压检测', '车内中控锁', '儿童座椅接口', '无钥匙启动', '防抱死系统(ABS)', '车身稳定控制(ESP)', '电动天窗', '全景天窗', '电动吸合门', '感应后备箱', '感应雨刷', '后雨刷', '前/后电动车窗', '后视镜电动调节', '后视镜加热', '多功能方向盘', '定速巡航', '后排独立空调', '空调控制方式', 'GPS导航', '倒车雷达', '倒车影像系统', '真皮座椅', '前/后排座椅加热']


def getData(url):
    # 获取详情页面信息
    resp = requests.get(url, headers=header)
    text = resp.content.decode('utf-8')
    html = etree.HTML(text)
    # 基础信息节点
    baseDom = '//div[@class="basic-infor js-basic-infor js-top"]'
    Xpath = {
        "标题": 'h2/span/text()',
        "车主": 'dl/dt/span/text()',
        "表显里程": 'ul/li[2]/div/text()',
        "变速箱": 'ul/li[4]/div/text()',
        "过户次数": 'ul/li[6]/div/text()',
        "看车方式": 'ul/li[7]/div/text()',
        "基本信息": 'div/table/tr/td/text()'
    }
    data = {}
    # 先获取ID和价格 ID是为了防止爬到同一条链接，数据重复
    data['ID'] = re.compile(
        r'^https://www.guazi.com/(.*).htm').match(url).group(1)
    data['价格'] = html.xpath(
        '/html/body/div[4]/div[3]/div[2]/div[1]/div[2]/span/text()')[0].strip()

    # 缩小匹配范围
    html = html.xpath(baseDom)[0]
    for key in Xpath:
        # 基本信息是一个表格数据，单独处理
        if key == "基本信息":
            arr = html.xpath(Xpath[key])
            # 把数组转成字典
            for i in range(0, len(arr), 2):
                # 因为导出是csv，所以不允许存在英文逗号
                data[arr[i]] = arr[i+1].replace(',','、')

        else:
            data[key] = html.xpath(Xpath[key])[0].strip().replace(',','、')

    return data


def init():
    # 检查链接是否已经全部爬取
    for item in brand:
        if not pathlib.Path("./data/" + item + "-Urls.csv").is_file():  # 不存在链接文件
            print('检测到详细页链接文件缺失, 请先执行【爬虫-获取详细页链接.py】, 程序退出...')
            # 强行退出程序
            sys.exit()

    # 读取上次进度 {执行到第几个城市，第几条数据，完成状态}
    global config
    try:
        with open('./data/getData.config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    except:
        with open('./data/getData.config.json', 'w', encoding='utf-8') as f:
            # 没有配置文件重新写一个
            config = {
                "brand": 0,
                "count": 0,
                "status": -1
            }
            f.write(json.dumps(config))

    if config["status"] != -1:
        print('检测到【获取详情页面信息】任务已全部完成, 程序退出...')
        # 强行退出程序
        sys.exit()


def saveProgress():
    # 覆盖存档
    with open('./data/getData.config.json', 'w', encoding='utf-8') as f:
        f.write(json.dumps(config))


def main():
    init()
    for brandIndex in range(config['brand'], len(brand)):
        config['brand'] = brandIndex
        brandName = brand[brandIndex]
        # 详细页链接列表
        urlList = pd.read_csv('./data/' + brandName + '-Urls.csv', header=None)[0]
        for urlIndex in range(config['count'], len(urlList)):
            print('正在爬取【{}】第{}条详细页数据'.format(brandName, urlIndex+1))
            with open('./data/未清洗数据.csv', 'a', encoding='utf-8') as f:
                # 没有内容 就先添加表头
                if f.tell() == 0:
                    f.write(','.join(infoKey)+'\n')
                try:
                    data = getData(urlList[urlIndex])
                    data['品牌'] = brandName
                    # 重新排序 存入csv
                    data = [data[item] for item in infoKey]
                    f.write(','.join(data)+'\n')

                except:
                    with open('./data/ERROR.csv', 'a', encoding='utf-8') as f:
                        f.write(brandName+','+str(urlIndex)+'\n')
                    print(brandName+','+str(urlIndex)+'错误')

            # 每读一页数据保存一下存档
            config['count'] = urlIndex
            saveProgress()

        # 每读完一个城市 重置计数 保存一下存档
        config['count'] = 0
        saveProgress()

    config["status"] = 0
    saveProgress()
    print('【获取详情页面信息】任务已全部完成, 程序退出...')


if __name__ == "__main__":
    main()

```

## 数据清洗

由于写的比较匆忙，随便找了点数据出来分析了

```python
import pandas as pd
import re

def dataFilter():
    data = pd.read_csv('./data/未清洗数据.csv')
    # 数据去重
    data = data.drop_duplicates(subset=["ID"], keep='first')

    # 数据清洗
    data["价格"] = data["价格"].map(lambda x: float(re.match( r'([0-9.]+)万', x).group(1)))
    data["标题"] = data["标题"].map(lambda x: x[:-5])
    data["表显里程"] = data["表显里程"].map(lambda x: re.match( r'([0-9.]+)万公里', x).group(1))
    data["过户次数"] = data["过户次数"].map(lambda x: re.match( r'([0-9]+)次过户', x).group(1))
    data["是否自动挡"] = data["变速箱"].map(lambda x: '自动' in x)
    data['车主性别'] = data["车主"].map(lambda x: ('先生' in x))

    # 要保留的数据
    columns = ["ID", "品牌", "价格", "标题", "表显里程", "过户次数", "车主性别",
                "变速箱","是否自动挡", "级别", "发动机", "燃料类型", "燃油标号", 
                "供油方式", "驱动方式"]
    data = pd.DataFrame(data, columns=columns)
    # 存储已清洗数据
    data.to_csv('./data/已清洗数据.csv')

def main():
    dataFilter()

if __name__ == "__main__":
    main()
```

## 数据可视化

没有使用 `pyecharts` ，直接写个html的 `echarts` 模板，然后通过文件读写的把js插入到html文档中，写完后发现有点蠢。

```html
<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>{{title}}</title>
<!-- <script src="https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js"></script> -->
<script src="js/echarts.min.js"></script>
<script src="js/walden.js"></script>
<style>
    #main{
        width: 1000px;
        height: 600px;
        margin: 0 auto;
    }
</style>
</head>
<body>
<!-- 为ECharts准备一个具备大小（宽高）的Dom -->
<div id="main"></div>

<script type="text/javascript">
    // 基于准备好的dom，初始化echarts实例
    var myChart = echarts.init(document.getElementById('main'), 'walden');
    // 指定图表的配置项和数据
    // 通过文件读写替换把代码内容插入
    // scriptSlot
    // 使用刚指定的配置项和数据显示图表。
    if (option && typeof option === "object") {
        myChart.setOption(option, true);
    }
</script>
</body>
</html>
```

```python
import pandas as pd
import json


def toEcharts(option):
    # 生成Echarts网页文件
    # option怎么配置？ https://echarts.apache.org/next/zh/tutorial.html
    title = option["title"]["text"]
    if title == '':
        return  # 因为使用title作文件名，空则退出
    try:
        with open('./web/template(请勿删除).html', 'r', encoding='utf-8') as f:
            html = f.read()
        # 写入网页标题
        html = html.replace("{{title}}", option["title"]["text"])
        # py布尔 转 js布尔
        option = str(option).replace('True', 'true').replace('False', 'false')
        # 写入js变量
        html = html.replace("// scriptSlot", 'var option = ' + option)
        with open(f'./web/{title}.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print(f'文件生成成功 ./web/{title}.html')
    except IOError:
        print('生成Echarts网页文件失败，template.html 模板文件缺失')

def 男女比例分析():
    男 = data[(data["车主性别"] == True)]
    女 = data[(data["车主性别"] == False)]
    男数量 = len(男)
    女数量 = len(女)
    # 根据品牌计数
    男 = 男["品牌"].value_counts()
    女 = 女["品牌"].value_counts()
    男数据 = []
    for index in range(len(男)):
        男数据.append( { "name": 男.index[index], "value": 男[index] } )
    女数据 = []
    for index in range(len(女)):
        女数据.append( { "name": 女.index[index], "value": 女[index] } )
    option = {
        "title": { "text": "车主男女比例分析", "subtext": "及各品牌中的男女占比", "x":'center'},
        "tooltip": { "trigger": "item", "formatter": "{a} <br/>{b}: {c} ({d}%)" },
        "legend": { "orient": "vertical", "left": 10, "data": brand, "y": "center" },
        "series": [
            {
                "name": "性别", "type": "pie", "selectedMode": "single", "radius": [ "0%", "20%" ], "label": { "position": "inner" }, "labelLine": { "show": False },
                "data": [ { "name": '男', "value": 男数量}, { "name": '女', "value": 女数量 } ]
            },
            { "name": "女占比", "type": "pie", "radius": [ "30%", "45%" ], "data": 女数据 },
             {
                "name": "男占比", "type": "pie", "radius": [ "65%", "80%" ],
                "label": { "formatter": "{a|{a}}{abg|}\n{hr|}\n  {b|{b}：}{c}  {per|{d}%}  ", "backgroundColor": "#eee", "borderColor": "#aaa", "borderWidth": 1, "borderRadius": 4, "rich": { "a": { "color": "#999", "lineHeight": 22, "align": "center" }, "hr": { "borderColor": "#aaa", "width": "100%", "borderWidth": 0.5, "height": 0 }, "b": { "fontSize": 16, "lineHeight": 33 }, "per": { "color": "#eee", "backgroundColor": "#334455", "padding": [2,4], "borderRadius": 2 } } },
                "data": 男数据
            }
        ]
    }
    toEcharts(option)

def 价格分析():
    # 各品牌价格分析
    # 最大值 最小值 平均值
    平均值 = []
    最大值 = []
    最小值 = []
    for item in brand:
        平均值.append(round(data[data["品牌"] == item]["价格"].mean(), 2))
        最大值.append(round(data[data["品牌"] == item]["价格"].max(), 2))
        最小值.append(round(data[data["品牌"] == item]["价格"].min(), 2))

    yAxis = brand + ["所有品牌(万元)"]
    平均值.append(round(data["价格"].mean(), 2))
    最大值.append(round(data["价格"].max(), 2))
    最小值.append(round(data["价格"].min(), 2))

    # print(平均值,最大值,最小值, yAxis)

    option = {
        "title": { "text": "二手车价格分析", "subtext":"数据来自瓜子二手车"},
        "tooltip": { "trigger": "axis", "axisPointer": { "type": "shadow" } },
        "legend": { "data": ["平均价格", "最高价格", "最低价格"] },
        "grid": { "left": "3%", "right": "4%", "bottom": "3%", "containLabel": True },
        "xAxis": { "type": "value"},
        "yAxis": { "type": "category", "data": yAxis },
        "series": [
            { "name": "平均价格", "type": "bar", "data": 平均值 },
            { "name": "最高价格", "type": "bar", "data": 最大值 },
            { "name": "最低价格", "type": "bar", "data": 最小值 },
        ]
    }
    toEcharts(option)

def 过户次数分析():
    平均过户次数 = round(data["过户次数"].mean(), 2)
    平均值 = {}
    for item in brand:
        平均值[item] = round(data[data["品牌"] == item]["过户次数"].mean(), 2)
    排序后的 = sorted(平均值.items(), key = lambda kv:(kv[1], kv[0]))[:5]
    X轴 = ["总平均过户次数"] + [item[0] for item in 排序后的]
    Y轴 = [平均过户次数] + [item[1] for item in 排序后的]
    # print(X轴,Y轴)
    option = {
        "title": { "text": "过户次数分析", "subtext":"平均过户量最少的5个品牌", "x":"center"},
        "color": ["#3398DB"], "tooltip": { "trigger": "axis", "axisPointer": { "type": "shadow" } }, "grid": { "left": "3%", "right": "4%", "bottom": "3%", "containLabel": True },
        "xAxis": [ { "type": "category", "data": X轴, "axisTick": { "alignWithLabel": True } } ], 
        "yAxis": [ { "type": "value" } ], "series": [ { "name": "直接访问", "type": "bar", "barWidth": "60%", "data": Y轴 } ]
    }
    toEcharts(option)

def 自动挡分析():
    # 自动挡占比 及变速箱排名
    是否自动档 = data["是否自动挡"].value_counts()
    自动档 = 是否自动档[True]
    手动档 = 是否自动档[False]
    变速箱前十 =  data["变速箱"].value_counts()
    变速箱其他 = 变速箱前十[10:].sum()
    变速箱前十 = 变速箱前十[0:9]
    变速箱前十名称 = 变速箱前十.index.tolist() + ['其他']
    变速箱前十 = 变速箱前十.values.tolist() + [变速箱其他]
    变速箱数据 = [{"name":变速箱前十名称[index], "value":变速箱前十[index]} for index in range(10)]
    # 逆序 展示更直观
    变速箱数据 = 变速箱数据[::-1]
    # print(变速箱数据,自动档, 手动档)
    option = {
        "title": { "text": "自动挡分析", "subtext": "及变速箱前十占比", "x":'center'},
        "tooltip": { "trigger": "item", "formatter": "{a} <br/>{b}: {c} ({d}%)" },
        "legend": { "orient": "vertical", "left": 10, "data": 变速箱前十名称, "y": "center" },
        "series": [
            {
                "name": "自动挡占比", "type": "pie", "selectedMode": "single", "radius": [ "0%", "30%" ], "label": { "position": "inner" }, "labelLine": { "show": False },
                "data": [ { "name": '自动档', "value": 自动档, "selected": True}, { "name": '手动档', "value": 手动档 } ]
            },
            {
                "name": "变速箱", "type": "pie", "radius": [ "40%", "55%" ],
                "label": { "formatter": "{a|{a}}{abg|}\n{hr|}\n  {b|{b}：}{c}  {per|{d}%}  ", "backgroundColor": "#eee", "borderColor": "#aaa", "borderWidth": 1, "borderRadius": 4, "rich": { "a": { "color": "#999", "lineHeight": 22, "align": "center" }, "hr": { "borderColor": "#aaa", "width": "100%", "borderWidth": 0.5, "height": 0 }, "b": { "fontSize": 16, "lineHeight": 33 }, "per": { "color": "#eee", "backgroundColor": "#334455", "padding": [2,4], "borderRadius": 2 } } },
                "data": 变速箱数据
            }
        ]
    }
    toEcharts(option)

def 其他参数分析(key, n):
    排序 = data[key].value_counts()[:n]
    X轴 = 排序.index.tolist()
    Y轴 = 排序.values.tolist()
    # print(X轴,Y轴)
    option = {
        "title": { "text": f"二手车-{key}-分析", "x":"center"},
        "color": ["#3398DB"], "tooltip": { "trigger": "axis", "axisPointer": { "type": "shadow" } }, "grid": { "left": "3%", "right": "4%", "bottom": "3%", "containLabel": True },
        "xAxis": [ { "type": "category", "data": X轴, "axisTick": { "alignWithLabel": True } } ], 
        "yAxis": [ { "type": "value" } ], "series": [ { "name": key, "type": "bar", "barWidth": "60%", "data": Y轴 } ]
    }
    toEcharts(option)


def main():
    global brand, data
    brand = ['大众', '本田', '别克', '丰田', '福特', '日产', '宝马',
         '奔驰', '现代', '奥迪', '雪佛兰', '吉利', '哈弗', '马自达', '起亚']
    data = pd.read_csv('./data/已清洗数据.csv')
    # 类型转换
    data["车主性别"] = data["车主性别"].astype("bool")
    data["价格"] = data["价格"].astype("float")
    data["表显里程"] = data["表显里程"].astype("int")
    data["过户次数"] = data["过户次数"].astype("float")
    data["是否自动挡"] = data["是否自动挡"].astype("bool")

    # 生成网页文件
    男女比例分析()
    价格分析()
    过户次数分析()
    自动挡分析()
    其他参数分析("级别", 10)
    其他参数分析("发动机", 5)
    其他参数分析("燃料类型", 3)
    其他参数分析("燃油标号", 2)
    其他参数分析("供油方式", 3)
    其他参数分析("驱动方式", 5)
    

if __name__ == "__main__":
    main()
```

![可视化结果图片](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201231111423023.png)

不得不说python写爬虫真的比其他编程语言高效，至于数据可视化我觉得应该写一套前端，然后用python存储分析结果，前端读取渲染可视化。