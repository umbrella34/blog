---
title: 爬虫实战-瓜子二手车
date: 2020-12-22 20:13:01
permalink: /pages/3c86e5/
categories: 
  - 技术
  - Python
tags: 
  - 
---
# 爬虫实战-瓜子二手车

这学期学python数据分析，数据分析首先要有数据。

通过简单的学习，写了个爬虫爬取瓜子二手车网站数据。

### 使用技术

软件环境：python3.8 + vscode + Anaconda

数据分析库包

- python网络爬虫：requests 和 lxml
- 数据分析：NumPy、Pandas、re
- 数据可视化：echarts

### 爬虫思路

- 先爬目录（详情页的链接）

  ![image-20201222212549581](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222212549581.png)

  ![image-20201222212928120](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222212928120.png)

- 访问详情页链接爬取数据

  ![image-20201222212750707](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222212750707.png)

- 把爬到的数据存入csv文件，你也可以存入到json、excel，甚至是数据库

  花了快1天，差不多爬了2.6w条数据

  ![image-20201222213034189](https://gitee.com/umbrella34/blogImage/raw/master/img/image-20201222213034189.png)

- 数据清洗与数据转换

- pandas，numpy 数据分析

- echarts可视化

### 遇到的问题：

- 最大页数只有50页

  网站显示有5w多条数据，但是分页只给50页，每页就40条数据，加起来才2000，远远达不到需要的量。首先我想到分城市来爬取目录，每个城市爬2000，然后发现换城市推荐的车基本都是重复的。后面改成分车的品牌来爬目录，勉强解决这个问题。但是这个网站没办法爬取完整数据。

- 爬虫中断续爬

  爬数据不是一朝一夕就能爬完的，中间也会遇到很多问题，比如断网了，cookie过期了，程序错误中断。总不能一错就重新开始爬吧，所以我通过读写json来保存最新进度。

  ```python
  def init():
      # 读取上次进度 {执行到第几个品牌，第几条数据，完成状态}
      global config
      try:
          with open('./data/getURL.config.json', 'r', encoding='utf-8') as f:
              config = json.load(f)
      except:
          with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
              # 没有配置文件重新写一个
              config = {
                  "brand": 0,
                  "count": 0,
                  "status": -1
              }
              f.write(json.dumps(config))
  
      if config["status"] != -1:
          print('检测到【获取详情页面Url】任务已全部完成, 程序退出...')
          # 强行退出程序
          sys.exit()
  
  
  def saveProgress():
      # 覆盖存档
      with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
          f.write(json.dumps(config))
          
  # 每写入一条数据，修改一下全局的config变量，再执行saveProgress函数
  # 如每爬完1页数据时就这样保存进度
  # 当品牌更换时记得把计数count重置为0
  config['count'] += 1
  saveProgress()
  
  ```

- 异常处理

  由于各种因素，程序不可避免错误，但也不能让它停止，所以使用`try except`来处理错误。因为网络问题，总有几条链接每成功访问，或者说爬到的链接已经过期。所以记录下错误的每一条链接位置，方便查找错误，后期也可以再写个程序捡回来重爬。

  ```
  try:
  	'...'
  except:
      with open('./data/ERROR.csv', 'a', encoding='utf-8') as f:
          f.write(某某品牌+','+str(第几行)+'\n')
  ```



## 项目源码-爬虫部分

```
项目结构

├── data      数据保存目录，请手动创建

├── 爬虫-获取详细页链接.py     爬取链接

├── 爬虫-获取详细页数据.py     爬取数据
```

```python
#爬虫-获取详细页链接.py

import requests # Python HTTP 库
import json # 读写进度
import sys # 用于中途退出程序
from lxml import etree # 用于解析字符串格式的HTML文档对象

# 请求头 自己登录一遍 F12 NetWork 复制过来
header = {
    "Cookie": "请重新获取",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
}

# 基础路径 瓜子二手车
BaseUrl = "https://www.guazi.com"

# 瓜子二手车 热门品牌中文和链接中的缩写
brand = ['大众','本田','别克','丰田','福特','日产','宝马','奔驰','现代','奥迪','雪佛兰','吉利','哈弗','马自达','起亚']
brandUrl = ['dazhong','honda','buick','toyota','ford','richan','bmw','benz','hyundai','audi','chevrolet','jili','hafu','mazda','kia']


def getUrls(url, pageIndex):
    # 获取详情页面url
    resp = requests.get(url, headers=header)
    text = resp.content.decode('utf-8')
    html = etree.HTML(text)
    maxPage = html.xpath('/html/body/div[6]/div[7]/ul/li/a/span/text()')[-2]
    # 超出最大页数返回None
    if pageIndex > int(maxPage):
        return None
    ul = html.xpath('//ul[@class="carlist clearfix js-top"]')[0]
    lis = ul.xpath('./li')
    urls = []
    for li in lis:
        url = li.xpath('./a/@href')
        url = BaseUrl+url[0]
        urls.append(url)
    return urls


def init():
    # 读取上次进度 {执行到第几个品牌，第几条数据，完成状态}
    global config
    try:
        with open('./data/getURL.config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    except:
        with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
            # 没有配置文件重新写一个
            config = {
                "brand": 0,
                "count": 0,
                "status": -1
            }
            f.write(json.dumps(config))

    if config["status"] != -1:
        print('检测到【获取详情页面Url】任务已全部完成, 程序退出...')
        # 强行退出程序
        sys.exit()


def saveProgress():
    # 覆盖存档
    with open('./data/getURL.config.json', 'w', encoding='utf-8') as f:
        f.write(json.dumps(config))


def main():
    init()
    for brandIndex in range(config['brand'], len(brand)):
        config['brand'] = brandIndex
        brandName = brand[brandIndex]
        for pageIndex in range(config['count'], 50):
            print('正在爬取【{}】第{}页数据链接'.format(brandName, pageIndex+1))
            with open('./data/{}-Urls.csv'.format(brandName), 'a', encoding='utf-8') as f:
                url = '{}/xm/{}/o{}/'.format(BaseUrl, brandUrl[brandIndex], pageIndex+1)
                data = getUrls(url, pageIndex+1)
                # 如果返回的是None 则改品牌没有50页 跳出
                if data is None:
                    break
                f.write('\n'.join(data)+'\n')
            # 每读一页数据保存一下存档
            config['count'] = pageIndex
            saveProgress()

        # 每读完一个城市 重置页数 保存一下存档
        config['count'] = 0
        saveProgress()

    config["status"] = 0
    saveProgress()
    print('【获取详情页面Url】任务已全部完成, 程序退出...')


if __name__ == "__main__":
    main()
```

```python
# 爬虫-获取详细页数据.py

import requests  # Python HTTP 库
import json  # 读写进度
import re  # 正则 用于截取链接部分作为ID
import sys  # 用于中途退出程序
import pathlib  # 用于判断文件是否存在
import pandas as pd  # 用于读取.csv链接数据
from lxml import etree  # 用于解析字符串格式的HTML文档对象

# 请求头 自己登录一遍 F12 NetWork 复制过来
# cookie 一段时间会过期 记的更换
header = {
    "Cookie": "请重新获取",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
}


# 瓜子二手车 热门品牌中文和链接中的缩写
brand = ['大众', '本田', '别克', '丰田', '福特', '日产', '宝马',
         '奔驰', '现代', '奥迪', '雪佛兰', '吉利', '哈弗', '马自达', '起亚']
brandUrl = ['dazhong', 'honda', 'buick', 'toyota', 'ford', 'richan', 'bmw',
            'benz', 'hyundai', 'audi', 'chevrolet', 'jili', 'hafu', 'mazda', 'kia']


# csv的表头
infoKey = ['ID', '价格', '品牌', '标题', '车主', '表显里程', '变速箱', '过户次数', '看车方式', '厂商', '级别', '发动机', '车身结构', '长/宽/高(mm)', '轴距(mm)', '行李箱容积(L)', '整备质量(kg)', '排量(L)', '进气形式', '气缸数', '最大马力(Ps)', '最大扭矩(N*m)', '燃料类型', '燃油标号', '供油方式', '驱动方式', '助力类型', '前悬挂类型', '后悬挂类型', '前制动类型', '后制动类型', '驱车制动类型', '前轮胎规格',
           '后轮胎规格', '主/副驾驶安全气囊', '前/后排侧气囊', '前/后排头部气囊', '胎压检测', '车内中控锁', '儿童座椅接口', '无钥匙启动', '防抱死系统(ABS)', '车身稳定控制(ESP)', '电动天窗', '全景天窗', '电动吸合门', '感应后备箱', '感应雨刷', '后雨刷', '前/后电动车窗', '后视镜电动调节', '后视镜加热', '多功能方向盘', '定速巡航', '后排独立空调', '空调控制方式', 'GPS导航', '倒车雷达', '倒车影像系统', '真皮座椅', '前/后排座椅加热']


def getData(url):
    # 获取详情页面信息
    resp = requests.get(url, headers=header)
    text = resp.content.decode('utf-8')
    html = etree.HTML(text)
    # 基础信息节点
    baseDom = '//div[@class="basic-infor js-basic-infor js-top"]'
    Xpath = {
        "标题": 'h2/span/text()',
        "车主": 'dl/dt/span/text()',
        "表显里程": 'ul/li[2]/div/text()',
        "变速箱": 'ul/li[4]/div/text()',
        "过户次数": 'ul/li[6]/div/text()',
        "看车方式": 'ul/li[7]/div/text()',
        "基本信息": 'div/table/tr/td/text()'
    }
    data = {}
    # 先获取ID和价格 ID是为了防止爬到同一条链接，数据重复
    data['ID'] = re.compile(
        r'^https://www.guazi.com/(.*).htm').match(url).group(1)
    data['价格'] = html.xpath(
        '/html/body/div[4]/div[3]/div[2]/div[1]/div[2]/span/text()')[0].strip()

    # 缩小匹配范围
    html = html.xpath(baseDom)[0]
    for key in Xpath:
        # 基本信息是一个表格数据，单独处理
        if key == "基本信息":
            arr = html.xpath(Xpath[key])
            # 把数组转成字典
            for i in range(0, len(arr), 2):
                data[arr[i]] = arr[i+1]

        else:
            data[key] = html.xpath(Xpath[key])[0].strip()

    return data


def init():
    # 检查链接是否已经全部爬取
    for item in brand:
        if not pathlib.Path("./data/" + item + "-Urls.csv").is_file():  # 不存在链接文件
            print('检测到详细页链接文件缺失, 请先执行【爬虫-获取详细页链接.py】, 程序退出...')
            # 强行退出程序
            sys.exit()

    # 读取上次进度 {执行到第几个城市，第几条数据，完成状态}
    global config
    try:
        with open('./data/getData.config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    except:
        with open('./data/getData.config.json', 'w', encoding='utf-8') as f:
            # 没有配置文件重新写一个
            config = {
                "brand": 0,
                "count": 0,
                "status": -1
            }
            f.write(json.dumps(config))

    if config["status"] != -1:
        print('检测到【获取详情页面信息】任务已全部完成, 程序退出...')
        # 强行退出程序
        sys.exit()


def saveProgress():
    # 覆盖存档
    with open('./data/getData.config.json', 'w', encoding='utf-8') as f:
        f.write(json.dumps(config))


def main():
    init()
    for brandIndex in range(config['brand'], len(brand)):
        config['brand'] = brandIndex
        brandName = brand[brandIndex]
        # 详细页链接列表
        urlList = pd.read_csv('./data/' + brandName + '-Urls.csv', header=None)[0]
        for urlIndex in range(config['count'], len(urlList)):
            print('正在爬取【{}】第{}条详细页数据'.format(brandName, urlIndex+1))
            with open('./data/未清洗数据.csv', 'a', encoding='utf-8') as f:
                # 没有内容 就先添加表头
                if f.tell() == 0:
                    f.write(','.join(infoKey)+'\n')
                try:
                    data = getData(urlList[urlIndex])
                    data['品牌'] = brandName
                    # 重新排序 存入csv
                    data = [data[item] for item in infoKey]
                    f.write(','.join(data)+'\n')

                except:
                    with open('./data/ERROR.csv', 'a', encoding='utf-8') as f:
                        f.write(brandName+','+str(urlIndex)+'\n')
                    print(brandName+','+str(urlIndex)+'错误')

            # 每读一页数据保存一下存档
            config['count'] = urlIndex
            saveProgress()

        # 每读完一个城市 重置计数 保存一下存档
        config['count'] = 0
        saveProgress()

    config["status"] = 0
    saveProgress()
    print('【获取详情页面信息】任务已全部完成, 程序退出...')


if __name__ == "__main__":
    main()

```

